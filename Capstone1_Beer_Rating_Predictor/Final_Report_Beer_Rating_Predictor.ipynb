{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone 1 Final Report: Beer Rating Predictor\n",
    "Written by Soham Desai\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"plots/beerbottle.png\" alt=\"ipa wordcloud\" style=\"height: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "1. Introduction\n",
    "2. Client Profile\n",
    "3. Data Information\n",
    "4. Data Wrangling\n",
    "5. Exploratory Data Analysis & Inferential Statistics\n",
    "6. Machine Learning\n",
    "7. Summary\n",
    "8. Recommendations\n",
    "9. Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picture you having a hard day at work where you were stressed over your head. On your way home you decide you deserve a little reward and want to unwind a little by having a beer or two. You stop by at the store or the local pub only to find a selection of beverages you have no idea about. What do you do? You can ask one or two people for a recommendation or better yet you can go search the Internet for reviews and ratings to make a more informed decision.\n",
    "\n",
    "With an increase in craft beers and breweries beer variety has increased dramatically. Due to this over saturation of products it can be very overwhelming to discover new brands and styles within the beer community. However, the Internet has brought beer connoisseurs from all around together on websites such as BeerAdvocate.com to discuss and review different products. In fact many consumers rely on these reviews to make informed purchases. Not only that, these reviews and ratings, when positive, can help businesses increase revenue by gaining recognition and reputation. It is therefore very valuable for, both breweries and connoisseurs, to have accurate ratings and reviews. I was curious to see if it is possible to predict ratings into two categories, high and low, based on the content of the review. Using Natural Language Processing and Sentiment Analysis I used various machine learning models to predict ratings from data I acquired. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 2. Client Profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My main client would be beer brewers at all levels. This information could be valuable for big corporations such as Budweiser all the way down to your local brewery that is about to start up. The machine learning algorithm could be used to sift through consumer reviews to provide the most accurate and consistent ratings to make informed decisions on beer in an effort to increase sales and customer satisfaction. It can also help shed light at the development phase by showing what styles are popular to see if a new combination could become a big seller.\n",
    "\n",
    "In addition, my algorithm can be expanded upon to be used for any business that has some sort of online database of reviews. With the proper parameters the algorithm can be altered to cater to any sort of business. They would be able to examine the content of the reviews and the derived sentiment to understand user emotions better and improve customer satisfaction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 3. Data Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains over 1.5 million reviews of various beers from two websites: BeerAdvocate.com. This data not only includes user reviews, product category and alcohol by volume(ABV), but sensory aspects as well such as taste, look, smell and overall ratings. For this project I will train and test models to predict beer ratings and beer style based off the user reviews that were left.\n",
    "\n",
    "These reviews were made available by Julian Mcauley, a UCSD Computer Science professor, from a collection period of January 1998 to November 2011. This dataset was accessed with permission. Here are some key specs of the dataset itself.\n",
    "\n",
    "+ Number of users: 33,387\n",
    "+ Number of items: 66,051\n",
    "+ Number of reviews: 1,586,259\n",
    "+ Rating Scales: Appearance, Aroma, Palate, Taste, Overall\n",
    "\n",
    "To tackle the issue of size, I took a subset sample of 99,999 reviews to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 4. Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After acquiring the data and examining it, there were several steps needed to be taken to clean the data before it was ready for analysis and machine learning models. \n",
    "\n",
    "\n",
    "__4.1.__ The different ratings left behind by users were on different scales. Aroma and Taste were on a scale of 10, while Appearance and Palate were on a scale of 5. The overall rating was also on an unknown scale. To remedy this, I scaled down \"Aroma\" and \"Taste\" down to a 5-point scale to match the other. I also combined the four sensory ratings and averaged them to get an \"Overall\" rating.\n",
    "\n",
    "__4.2.__ The text reviews had a bunch of miscellaneous information in them. This included spelling errors, punctuation, stop words, contractions, capitalization and whitespace. Also, having different tenses of a word is not necessary. For example the words plays, played, playing all come from the base word play. I only wanted the base word. To clean the text, I created a bunch of functions to lowercase the text, expand contractions, remove whitespace and remove stop words. In addition I used two common practices in NLP, stemming and lemmatization to derive the base of each word. Although lemmatization took more time, I found it more effective and chose too keep the lemmatized text reviews over the stemmed ones.\n",
    "\n",
    "__4.3.__ Non-English Text Reviews. Some of the text reviews were not in English. This is an issue because the non-English words would throw the model off. Using lang detect I searched through the data and dropped any rows of text reviews that were not identified as English. Sometimes it was unable to detect the language. For these I used a try and except clause to ignore the reviews that a language could not be identified.\n",
    "\n",
    "To wrap up data wrangling I also did the following to the dataset:\n",
    "+ Took a subset of the entire data collected\n",
    "+ Dropped unnecessary columns: time, beerId, brewerId\n",
    "+ Fixed spelling errors\n",
    "+ Broke down text reviews in preparation for sentiment analysis\n",
    "+ Added a new feature: word_count\n",
    "+ Extracted five beer styles to examine along with the entire dataset: Amber Ale, Belgian Ale, Brown Ale, IPA, Stout\n",
    "+ Started with dataset size of 99,999. After cleaning and removing data that it was reduced to 81048.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 5. Exploratory Data Analysis and Inferential Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5.1. Rating Scales__\n",
    "\n",
    "In total there were 5 rating scales provided from the dataset acquired: Appearance, Aroma, Palate, Taste and Overall. Each one provides its own insight on different qualities of a beer, but for the purpose of this project I focused on the Overall rating by reviewers to get, as the rating is called, \"Overall\" understanding. In addition to examining the entire dataset I selected five beer styles to look at to see if there were any oddities.\n",
    "\n",
    "<img src=\"plots/overall_ratings.png\" alt=\"overall ratings\"/>\n",
    "\n",
    "__Figure 1:__ This shows the distribution of ratings for the entire beer dataset(gold) along with five beer styles: India Pale Ale, Amber Ale, Belgian Ale, Brown Ale and Stout. The white square in the middle of each box plot represents the average amongst all ratings. You can see that for the entire dataset, both the mean and median fall roughly around 3.5. This varies amongst styles of beers but still remains within the 3-4 star range. \n",
    "\n",
    "Since the ratings ranged within the 3-4 star range, I decided to create a binary rating system derived from this plot. Reviews with an overall rating less than 3.5 were considered to be \"low\" and reviews greater than or equal to 3.5 were considered to be \"high\". \n",
    "\n",
    "The most important feature for the model is the text within each review. To further examine this I decided to create a new feature, word count, and examine this further. Does the review length help determine anything within this newly created two-rating system. On initial appearances it appeared that the higher the rating given, the lengthier the review was in terms of words. To further answer this question I performed a two sample t-test for unequal variances. Results showed that the word count of reviews rated higher than 3.5 is significantly higher than reviews rated less than 3.5. \n",
    "\n",
    "<img src=\"plots/high_low_words.png\" alt=\"word_frequency_1\"/>\n",
    "\n",
    "__Figure 2:__ Word counts for reviews greater than 3.5 are higher than word counts for reviews less than 3.5 for the entire dataset and each beer style examined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5.2. Text Reviews__\n",
    "\n",
    "<img src=\"plots/wordfreq.png\" alt=\"word_frequency_1\"/>\n",
    "\n",
    "__Figure 3:__ Top 25 most common words appearing within the dataset.\n",
    "\n",
    "As mentioned earlier, the main features of the model will come from the text reviews. To further examine this I looked at the 25 most common words within the entire corpus. Among this list there are sensory words (aroma, sweet, light, caramel, etc.) that help imagine the beer. There are also words in relation to beer such as \"hop\", \"head\" (in reference to the foam on the top of a beer) and \"pour\". Although text preprocessing was already performed, additional words may need to be removed such as \"beer\" because they do not help in distinguishing reviews from one another. \n",
    "\n",
    "A point of notice is that the word \"not\" appeared quite frequently. When determining sentiment the word \"not\" can change the polarity of the review. For example the difference in the sentences \"This is good\" and \"This is not good\". Below I have  the probability for some words associated with a high and low overall rating. Individual words are in the first set of probabilities. You see words that you would normally see with high and low reviews such as 'excellent' and 'delicious', but you also see noise such as 'xx'. The probabilities of each word are also not that strong.\n",
    "\n",
    "***1 WORD***\n",
    "\n",
    "High Overall Words\t     P(high | word)        \n",
    "+           excellent 0.92\n",
    "+              chimay 0.89\n",
    "+           beautiful 0.88\n",
    "+           delicious 0.87\n",
    "+                rich 0.87\n",
    "+             complex 0.86\n",
    "+                full 0.86\n",
    "+                pine 0.86\n",
    "+          grapefruit 0.86\n",
    "+           wonderful 0.86\n",
    "\n",
    "Low Overall Words\t     P(low | word)\n",
    "+              boring 0.39\n",
    "+           cardboard 0.38\n",
    "+                  xx 0.38\n",
    "+               stale 0.37\n",
    "+              skunky 0.36\n",
    "+               lager 0.36\n",
    "+              watery 0.34\n",
    "+                weak 0.34\n",
    "+                corn 0.30\n",
    "+               bland 0.29\n",
    "\n",
    "In comparison when you look at both one and two word phrases, you begin to get stronger probabilities. In addition it can tell you a lot about what contributes into a a good beer from the eyes of the consumers. For example looking at the high and low words below you can see that customers value \"smooth\", \"really solid\" and \"perfectly carbonated\" beers. These are good hints to keep in mind when thinking of the next beer to create. \n",
    "\n",
    "Likewise you can also see things to avoid such as grainy, thin or watery flavors to the beer. Creativity and thinking outside the box can lead to amazing new discoveries in flavor profiles, but it can also lead to disastrous failure. Here you can see that the flavor combination of raspberry and wheat wast that favored well. Just like this you can obtain a lot of information from the predictive word features. Let's see if looking at a few styles of beer can help shed some insight. \n",
    "\n",
    "***1-2 WORDS***\n",
    "\n",
    "High Overall Words\t     P(high | word)\n",
    "+   incredibly smooth 0.98\n",
    "+           tasty ipa 0.98\n",
    "+            superbly 0.98\n",
    "+          year round 0.98\n",
    "+         great color 0.98\n",
    "+      great trappist 0.98\n",
    "+        new favorite 0.98\n",
    "+          full thick 0.98\n",
    "+ perfect carbonation 0.98\n",
    "+        really solid 0.98\n",
    "\n",
    "Low Overall Words\t     P(low | word)\n",
    "+      flavour watery 0.02\n",
    "+        bland boring 0.02\n",
    "+                bleh 0.02\n",
    "+     wheat raspberry 0.02\n",
    "+       buttery aroma 0.02\n",
    "+          quack duck 0.02\n",
    "+       flavour grain 0.02\n",
    "+        no character 0.02\n",
    "+        flavour thin 0.02\n",
    "+              famosa 0.02\n",
    "              \n",
    "              \n",
    "Since \"tasty ipa\" appeared in the \"high\" overall words, let's begin by looking at __IPAs__. The predictive features for this style of beer are below. \n",
    "\n",
    "High Overall Words\t     P(high | word)\n",
    "+         medium malt 0.98\n",
    "+           great hop 0.98\n",
    "+    grapefruit lemon 0.98\n",
    "+           tasty ipa 0.98\n",
    "+        fruit citrus 0.98\n",
    "+           head huge 0.98\n",
    "+           hop great 0.98\n",
    "+       favorite beer 0.98\n",
    "+         nice finish 0.97\n",
    "+           good ipas 0.97\n",
    "\n",
    "Low Overall Words\t     P(low | word)\n",
    "+            lack hop 0.12\n",
    "+        not terrible 0.10\n",
    "+            weak ipa 0.08\n",
    "+         faint aroma 0.07\n",
    "+                seat 0.07\n",
    "+           back seat 0.07\n",
    "+            hope not 0.06\n",
    "+             oxidize 0.06\n",
    "+       underwhelming 0.05\n",
    "+         soapy taste 0.04\n",
    "         \n",
    "Here you can see that for IPAs, consumers enjoyed a beer that had \"great hops\" and a \"nice finish\". Accents of flavor such as \"grapefruit lemon\" and \"citrus\" were also big compliments. On the other side you can see that the \"lack of hops\" and unusual flavors (\"soapy taste\") were good indicators to a poor IPA. Looking at predictive features can help gain insight on what to create next and what the population enjoys.\n",
    "\n",
    "After going to Munich, I could always go for a solid __German Hefeweizen__, a wheat beer. To satisfy my curiosity let's look at the predictive features for this beer as well.\n",
    "\n",
    "High Overall Words\t     P(high | word)\n",
    "+               fresh 0.60\n",
    "+               blind 0.59\n",
    "+       standard hefe 0.59\n",
    "+             mixture 0.59\n",
    "+          augustiner 0.59\n",
    "+     refreshing nice 0.59\n",
    "+           excellent 0.58\n",
    "+      mouthfeel nice 0.58\n",
    "+                 egy 0.58\n",
    "+             amazing 0.58\n",
    "\n",
    "Low Overall Words\t     P(low | word)\n",
    "+             no head 0.17\n",
    "+          head wheat 0.17\n",
    "+        yellow small 0.16\n",
    "+             sourish 0.16\n",
    "+             nothing 0.16\n",
    "+          dry wheaty 0.15\n",
    "+                 bad 0.15\n",
    "+           bottle cl 0.15\n",
    "+            not good 0.13\n",
    "+                weak 0.10\n",
    "                \n",
    "A solid hef should be easy to drink and have almost like a bananaish flavor to it. When poured it should have a nice layer of foam at the top created from the carbonation of the beer. You can see in the words for \"low\" reviews mention abnormalities from what you'd expect usually. The beer should not be \"sourish\" and it should not have \"no head\". It should be \"refreshing\" and the consistency of the beer should \"feel nice\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 6. Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine learning model aims to take new text reviews and predict either a low or high overall rating. Normally you'd first want to do some text preprocessing to clean the text reviews of nonessential information such as contractions, punctuation and whitespace, but I had already done this when first cleaning the dataset. The following steps were then taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6.1. Vectorizer Selection__\n",
    "Vectorizers encode text data and depending on which method you choose, it is calculated differently. I decided to use two different ones, CountVectorizer and TfidfVectorizer, which are both part of Scikit-learn. \n",
    "1. __CountVectorizer:__ uses a \"bag-of-words\" approach where the text is analyzed by word counts. It counts the occurrence of each token for each review.\n",
    "2. __TfidfVectorizer:__ uses the term frequency(tf) and the inverse document frequency(idf) to create weighted term frequencies. To make more sense of this here is an example. If the word \"IPA\" shows up in all the documents, it wouldn't be that helpful for predictions. This vectorizer will downweight the predictive value of the word \"IPA\" because it does not help us as much. \n",
    "\n",
    "To compare the two vectorizers I applied both vectorizers separately to a simple Naive Bayes classifier, MultinomialNB() including bigrams (ngram_range = (1,2)). I calculated the ROC-AUC score to compare the models. \n",
    "\n",
    "After I changed the minimum document frequency to .0001 instead of 1. I then used GridSearchCV to select the best parameters for the simple Naive Bayes model. This told me that the default parameters for alpha and fit_prior were the best. I then recalculated the ROC-AUC score for both vectorizers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Vectorizer             | ROC-AUC | Best Parameters                       |\n",
    "|------------------------|---------|---------------------------------------|\n",
    "| CountVectorizer        | 0.853   | min_df=1, alpha=1, fit_prior=True     |\n",
    "| TfidfVectorizer        | 0.848   | min_df=1, alpha=1, fit_prior=True     |\n",
    "| CountVec w/ GridSearch | 0.853   | min_df=.0001, alpha=1, fit_prior=True |\n",
    "| TfidfVec w/ GridSearch | 0.857   | min_df=.0001, alpha=1, fit_prior=True |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Table 1:__ Comparison of vectorizers with a Multinomial Naive Bayes Model with and without hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the table above you can see that the TfidfVectorizer worked best with tuned parameters. The following parameters were best and what I used moving forward on other models and classifiers.\n",
    "\n",
    "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
    "       estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
    "       fit_params=None, iid='warn', n_jobs=-1,\n",
    "       param_grid={'fit_prior': (True, False), 'alpha': (0.001, 0.01, 0.1, 1, 5, 10)},\n",
    "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
    "       scoring='roc_auc', verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6.2. Model Comparison__\n",
    "\n",
    "Using the TfidfVectorizer I fit and tuned four classifiers: Logistic Regression, Support Vector Machines, Multinomial Naive Bayes and Random Forest Trees. Each classifier used an ngram_range of (1,2) and a min_df of 0.0001. I used area under the curve(ROC) to compare the models to one another. For Random Forest Trees, and Naive Bayes I used GridSearchCV for cross-validation and hyper parameter tuning. For LogisticRegression I used an algorithm(LogisticRegressionCV) that included cross-validation. Lastly, using SVM, I was able to get the second highest ROC score, but I decided not to work with it further for the time being due to the amount of time it took to run the model. \n",
    "\n",
    "The highest scoring model turned out to be Logistic Regression. The score and parameters of each model are in the table below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Classifier             | ROC-AUC | Best Parameters                       |\n",
    "|------------------------|---------|---------------------------------------|\n",
    "| MultinomialNB          | 0.859   | alpha=1, fit_prior=True               |\n",
    "| LogisticRegressionCV   | 0.873   | Cs=10, max_iter=1000                  |\n",
    "| RandomForestClassifier | 0.851   | max_features=500, min_samples_leaf=5  |\n",
    "| SVC                    | 0.868   | C=1, kernal='linear', degree=3        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Table 2:__ Comparison of four machine learning models fitted with TfidfVectorizer. \n",
    "\n",
    "<img src=\"plots/roc_curve_logreg.png\" alt=\"word_frequency_1\"/>\n",
    "\n",
    "__Figure 4.__ ROC curve for best model, LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6.3. Examine Text, False Positives, False Negatives and Predicted Probabilities__\n",
    "\n",
    "In hopes of further improving the model, I decided to examine the text in more detail. I wanted to see if I could find common characteristics within false positives and false negatives that could be addressed to reduce their occurrences. To do so I examined a couple false positives with the highest predicted probability and a couple false negatives with the lowest predicted probabilities. This way I would be able to examine the extremes of both error groups. Below are two reviews from each. \n",
    "\n",
    "1. False Positives\n",
    "    + 22oz bottle from Bevmo.  Pours a yellowish color with a pure white, fluffy head. Piney citrus aroma. Nice toasty malt flavors behind some citrus hop flavors. Very floral in the finish. Really nice and drinkable, this one has some good character, and hides the alcohol well. A very solid, tasty brew. \n",
    "    + fruity sweet smell with a nice foamy head. good malt with a dry hoppy finish good balance alcohol is well hidden \n",
    "\n",
    "\n",
    "2. False Negatives\n",
    "    + On tap, at brewpub.  Pours a clear pale yellow with a minimal wispy white head, light visible carbonation.  Light but clean pale malt nose, no corn noticeable with minimal to no hops.  The flavors a bit grassy and has a touch of corn, but its still pretty clean.  The mouth feel is light bodied and fizzy wth fizzy spritzy carbonation.  Not bad for what it is. \n",
    "    + 650 mL bottle.  Looks the part, but the aroma is mild, and the flavour is one-dimensional.  Not enough roast and too much soya sauce.  Not bad, really.  Just a little disappointing. \n",
    "\n",
    "For most of each review, the information discussed describes the beer and its qualities, talking about color, taste, appearance, carbonation, etc. However there are portions in each that provide some form of sentiment with words like \"good\" and \"solid\" in the false positives and words like \"disappointing\" and \"not enough\". This may make the reviews appear more negative or positive than they really are.\n",
    "\n",
    "Second, it could be that these reviewers are either very harsh critics or more easy going. Since my binary rating of \"high\" and \"low\" was made at the point 3.5, there may be some who consider an average beer to be 3 stars and other who would put it up at 4. With more data it may be wise to leave the rating system unadjusted. To see if this issue really made a difference, I looked at extreme false positives and negatives. By this I mean that a false negative that actually had an overall rating of 5 and a false positive that had an actual overall rating of 1.\n",
    "\n",
    "1. Extreme False Positives\n",
    "    + Nice taste for its genre. But is not the typical white beer such as Hoegaarden,...\t\tI prefer the dark Chimay. Much better!!! \n",
    "    + 12oz draft from the harp... It tastes like that blueberry wheat stuff from seadog that everyone loves.  It appears that american wheat beers are now being fruited up - in an attempt to capture some of the malternatives market.  Even people who stick to harpoon, snpa, and long trail know that this beer is crap like #9 and blueberry wheats. \n",
    "\n",
    "\n",
    "Look at these extreme false positives you can see why it might be hard for the model to predict this accurately. The first review has a lot of positive words such as \"nice taste\", \"much better\", \"prefer\", but these are all being used to reference another beer, not the one being reviewed. Similarly, the second review references other beers and brands that may be causing the model to predict this rating incorrectly.  \n",
    "\n",
    "\n",
    "2. Extreme False Negatives\n",
    "    + I have scoured the world and found no better beer. Complexity abounds. There is a distinctly different start, middle, and finish which  goes through sweet, dry, fruity, bitter, and malt. \n",
    "    + Pours a darkbrown  beer, a little cloudy with some proteine chunks floating around. Off-white head that clears fast (dirty glass?). Smells like caramel and coffee, ripe banana. A prickling mouthfeeling. First flavour is pretty sweet, caramel, chocolate and roasted malts. Ends like sour apple, brown sugar yet not sticky, cinamon. Aftertasty is pleasant hoppy.\n",
    "\n",
    "\n",
    "Both of these reviews do not provide a lot of sentiment. For the most part it is descriptive of the beer and its qualities. The model maybe confused with words such as \"prickling\" and \"dirty glass\" that describe more of the experience rather than the beer itself.\n",
    "\n",
    "In addition I examined the predicted probabilities of some of these reviews that were predicted incorrectly in the table below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Incorrect Prediction | Index | Actual Rating | Predicted Probability |\n",
    "|----------------------|-------|---------------|-----------------------|\n",
    "| False Positive       | 67135 | low           |0.995780               |\n",
    "| False Positive       | 61414 | low           |0.993783               |\n",
    "| False Positive       | 2222  | low           |0.989848               |\n",
    "| False Negative       | 36260 | high          |0.004538               |\n",
    "| False Negative       | 4757  | high          |0.005078               |\n",
    "| False Negative       | 55204 | high          |0.005502               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Table 3:__ This table shows the false prediction of a review, the index it can be found in the data, the correct rating and the predicted probability using LogisticRegressionCV. \n",
    "\n",
    "The default threshold for the predicted probabilities is 0.5. The model predicts the class that has a probability greater than 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6.4. Adjusting Threshold of Logistic Regression Model for Different Business Models.__\n",
    "\n",
    "Furthermore, another way to examine the performance of the model is to look at the confusion matrix. For the best model, LogisticRegressionCV at a threshold level of 0.5, 9982 reviews were correctly predicted to be 'high' ratings and 6229 were correctly predicted to be 'low' ratings. Likewise, 1762 were incorrectly predicted to be 'high' ratings (false positives) and 2379 were incorrectly predicted to be 'low' ratings (false negatives). \n",
    "\n",
    "However, in real life situations you may require the model to focus on different metrics. You may want the model to be as accurate as it can be or you may determine that focus needs to be put on one category over another. What type of scoring metrics could you use? Well, I used the following metrics.\n",
    "+ Sensitivity (Recall) - The proportion of positives that are correctly predicted \n",
    "+ Precision - The proportion that the prediction will be a positive\n",
    "+ Accuracy - The proportion correct predictions \n",
    "\n",
    "In addition, because the dataset was imbalanced, I looked at the following two metrics to take account for this.\n",
    "+ F1 Score - The harmonic mean between precision and recall.\n",
    "+ Balanced Accuracy - Accuracy determined for imbalanced datasets. \n",
    "\n",
    "These metrics are important for three business cases that could be applied to the beer reviews. They can be further broadened if desired to apply to any business that involves user created reviews.\n",
    "\n",
    "__BUSINESS CASE 1: Beer Market Comparisons__\n",
    "\n",
    "Let's say you've put a new beer onto the market and you're curious how well it's doing on its own and in comparison to other beers you've launched. You want an aggregate score that will tell you the percentage of high reviews and percentage of low reviews on however many new remarks were left behind. To measure this you'd want the model to be as accurate as it can be. For this reason, the metric I decided to use for this was __accuracy__. Since the dataset is unbalanced, there are more \"high\" overall ratings than \"low\", I also examined __balanced-accuracy__. \n",
    "\n",
    "When taking into account the unbalanced dataset, the best threshold is the default value of 0.5. It is 79% accurate. However, if you want to obtain the most accurate prediction regardless of the imbalance you would want to move the threshold to 0.43. The accuracy improves to 80%, reducing the total number of mispredictions to 4007 from 4147. A key point to make is that although the overall number of mispredictions is reduced, the number of false positives increases and the number of false negatives decreases. The values are described in the table below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Prediction Metric | % Accurate | True Positive | True Negative | False Positive | False Negative |\n",
    "|-------------------|------------|---------------|---------------|----------------|----------------|\n",
    "| Accuracy          | 0.803      | 10537         | 5808          | 2183           | 1824           |\n",
    "| Balanced Accuracy | 0.794      | 9982          | 6229          | 1762           | 2379           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Table 4:__ This provides the confusion matrix and score of the metrics Accuracy and Balanced Accuracy\n",
    "\n",
    "To visually understand this I've also plotted this.\n",
    "\n",
    "<img src=\"plots/accuracy_plot.png\" alt=\"accuracy plot\"/>\n",
    "\n",
    "__Figure 5.__ Accuracy Curve for LogisticRegressionCV with different thresholds\n",
    "\n",
    "__BUSINESS CASE 2: Customer Satisfaction__\n",
    "\n",
    "Another possibility could be that you're looking to improve customer satisfaction. You'd need to target users who have written reviews with \"low\" overall ratings so that you can examine what they're saying about a specific beer and if there is anything that can be done to change their opinion on the current or future beers to come. To measure this, I decided to use the F1-Score because it is a better measure when focusing on a specific class, in this case the \"low\" overall ratings. In addition it takes into account both, precision and recall, meaning the higher the F1-Score the better the model is performing. \n",
    "\n",
    "After examining various thresholds, the optimal threshold for \"low\" ratings is at 0.54. It outputs an F1-Score of 0.75. \n",
    "\n",
    "In comparison looking at the opposite, the optimal threshold for \"high\" ratings you notice that the change is greater from 0.5 to 0.34. The F1-Score also is higher when optimizing for high ratings at a score of 0.82. This can be seen in the plot and table below.\n",
    "\n",
    "<img src=\"plots/ratings_plot.png\" alt=\"accuracy plot\"/>\n",
    "\n",
    "__Figure 6.__ Precision, Recall and F1 Curves with Thresholding for \"low\" and \"high\" ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Case         | Threshold Level | Precision | Recall | F1-Score |\n",
    "|--------------|-----------------|-----------|--------|----------|\n",
    "| Default      | 0.50            | .85       | .81    | .83      |\n",
    "| Low Ratings  | 0.54            | .70       | .78    | .82      |\n",
    "| High Ratings | 0.34            | .80       | .90    | .84      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 7. Summary\n",
    "\n",
    "Simply looking at the predictive features of the model can tell you a lot about beers. Based on the data, consumers valued an IPA that had great hops, a medium to malty taste, and fruity accents like citrus and grapefruit within the beer. In comparison if you are to look at German Hefeweizens, consumers valued a more fresh and refreshing taste along with the beer having a layer of foam at the top of the glass. By splitting the data by beer brand, style, similarities, etc. you can find information about the qualities of a beer that people value. Beer can vary in flavor, appearance, taste, smell, etc. and it is only getting more complex as more people continue to create their own concoctions. \n",
    "\n",
    "To further examine the text I looked at false negatives and false positives and their predicted probabilities. In addition I examined different thresholds to optimize correct predictions for both 'low' and 'high' ratings. What I found was that there were quite a lot false negative predictions(2379) and false positive predictions(1762). To continue to explore I decided to examine two different business cases by thresholding.\n",
    "\n",
    "The default threshold was 0.5 meaning any value above that was considered a 'high' rating and any value below was considered 'low'. However, this can be adjusted depending on what you are measuring for. \n",
    "\n",
    "In Business Case 1, the model was optimized to provide an aggregate score allowing you to be able to identify the percentage of positive and negative reviews left behind for a specific beer or group of beers. In order to measure this, I used the metric of balanced_accuracy. To do so I adjusted the threshold to 0.43 scoring at 80% accuracy. \n",
    "\n",
    "In Business Case 2, I wanted to make the model optimize low ratings. Why? Low ratings mean there is room for improvement. By identifying what consumers are saying is wrong about a product, a company can reach out these peoples in hopes to understand the issue and to create a resolution to change their opinion. Doing so I used the F1-Score as a metric. It takes into account both precision and recall so that neither side is neglected, but instead optimized for the precision-recall tradeoff. Again, I adjusted the threshold from 0.5 to 0.54 outputting a F1-Score of 0.75. \n",
    "\n",
    "\n",
    "In sum if you'd like a quick summary of what this model can do for you, I'll list them below:\n",
    "+ You can look at the predictive features of the model to see what words or word combinations consumers are using to describe a \"high\" rating beer vs a \"low\" rating beer. You can split the beers into smaller groups by style (IPAs, Hefeweizen, Red Ale, etc.), or even your own categories where you are combining some styles together.\n",
    "+ You need an optimized model to perform for most accurate predictions. (Business Case 1)\n",
    "+ You need a model that can be optimized to focus on a specific class, in this case \"low\" ratings. (Business Case 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 8. Recommendations for Future Implementation\n",
    "\n",
    "To conclude my project, I used Logistic Regression to correctly predict as many 'high' and 'low' ratings that I could using cross-validation, thresholding, bi-grams, vectorizers and much more. However, there is still a lot of room for improvement and with the time and resources there are many ways this model can be made to be much better. Here are a few of these topics below.\n",
    "+ Use all data in the dataset. I only used 99,999 reviews throughout the project but the dataset has an additional 1.4 million reviews. Also, use more recent data from any website with beer reviews to match more current events. \n",
    "+ Try and focus on various styles of beers. I examined the IPA beer dataset to see if the overall model would work better on an individual dataset, but it failed to do so. However, there are over a 100 more different styles of beer to be explored and examined. An adjusted model for each style may create an overall better rating predictor.\n",
    "+ There are four other rating scales that can be explored as well. I focused on the \"overall\" rating to get a general understanding of the review,s but there are still \"appearance\", \"aroma\", \"palate\", and \"taste\". In addition I reduced the rating scale from a 5 point scale to a binary scale. With more data it may be a good idea to examine it on the original scale it came on.\n",
    "+ Explore word connections some more\n",
    "    + There are more advanced NLP practices such as Word2Vec that can provide more insight from the text reviews\n",
    "+ I mentioned that I had to stop working with SVC due to time constraints. With more processing power it may be interesting to explore other and more powerful models. It would help work with a bigger dataset as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 9. Sources\n",
    "\n",
    "Lipton, Zachary & Vikram, Sharad & McAuley, Julian. (2015). Capturing Meaning in Product Reviews with Character-Level Generative Text Models.\n",
    "https://www.researchgate.net/publication/283761921_Capturing_Meaning_in_Product_Reviews_with_Character-Level_Generative_Text_Models\n",
    "\n",
    "Springboard Mentors - Benjamin Bell, DJ Sarkar, Kenneth Gil"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
