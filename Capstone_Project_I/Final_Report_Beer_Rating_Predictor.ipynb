{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone 1 Final Report: Beer Rating Predictor\n",
    "Written by Soham Desai\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"plots/beerbottle.png\" alt=\"ipa wordcloud\" style=\"height: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "1. Introduction\n",
    "2. Client Profile\n",
    "3. Data Information\n",
    "4. Data Wrangling\n",
    "5. Exploratory Data Analysis & Inferential Statistics\n",
    "6. Machine Learning\n",
    "7. Summary\n",
    "8. Recommendations\n",
    "9. Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picture you having a hard day at work where you were stressed over your head. On your way home you decide you deserve a little reward and want to unwind a little by having a beer or two. You stop by at the store or the local pub only to find a selection of beverages you have no idea about. What do you do? You can ask one or two people for a recommendation or better yet you can go search the Internet for reviews and ratings to make a more informed decision.\n",
    "\n",
    "With an increase in craft beers and breweries beer variety has increased dramatically. Due to this over saturation of products it can be very overwhelming to discover new brands and styles within the beer community. However, the Internet has brought beer connoisseurs from all around together on websites such as BeerAdvocate.com to discuss and review different products. In fact many consumers rely on these reviews to make informed purchases. Not only that, these reviews and ratings, when positive, can help businesses increase revenue by gaining recognition and reputation. It is therefore very valuable for, both breweries and connoisseurs, to have accurate ratings and reviews. I was curious to see if it is possible to predict ratings into two categories, high and low, based on the content of the review. Using Natural Language Processing and Sentiment Analysis I used various machine learning models to predict ratings from data I acquired. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 2. Client Profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My main client would be beer brewers at all levels. This information could be valuable for big corporations such as Budweiser all the way down to your local brewery that is about to start up. The machine learning algorithm could be used to sift through consumer reviews to provide the most accurate and consistent ratings to make informed decisions on beer in an effort to increase sales and customer satisfaction. It can also help shed light at the development phase by showing what styles are popular to see if a new combination could become a big seller.\n",
    "\n",
    "In addition, my algorithm can be expanded upon to be used for any business that has some sort of online database of reviews. With the proper parameters the algorithm can be altered to cater to any sort of business. They would be able to examine the content of the reviews and the derived sentiment to understand user emotions better and improve customer satisfaction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 3. Data Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains over 1.5 million reviews of various beers from two websites: BeerAdvocate.com. This data not only includes user reviews, product category and alcohol by volume(ABV), but sensory aspects as well such as taste, look, smell and overall ratings. For this project I will train and test models to predict beer ratings and beer style based off the user reviews that were left.\n",
    "\n",
    "These reviews were made available by Julian Mcauley, a UCSD Computer Science professor, from a collection period of January 1998 to November 2011. This dataset was accessed with permission. Here are some key specs of the dataset itself.\n",
    "\n",
    "+ Number of users: 33,387\n",
    "+ Number of items: 66,051\n",
    "+ Number of reviews: 1,586,259\n",
    "+ Rating Scales: Appearance, Aroma, Palate, Taste, Overall\n",
    "\n",
    "To tackle the issue of size, I took a subset sample of 99,999 reviews to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 4. Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After acquiring the data and examining it, there were several steps needed to be taken to clean the data before it was ready for analysis and machine learning models. \n",
    "\n",
    "\n",
    "__4.1.__ The different ratings left behind by users were on different scales. Aroma and Taste were on a scale of 10, while Appearance and Palate were on a scale of 5. The overall rating was also on an unknown scale. To remedy this, I scaled down \"Aroma\" and \"Taste\" down to a 5-point scale to match the other. I also combined the four sensory ratings and averaged them to get an \"Overall\" rating.\n",
    "\n",
    "__4.2.__ The text reviews had a bunch of miscellaneous information in them. This included spelling errors, punctuation, stop words, contractions, capitalization and whitespace. Also, having different tenses of a word is not necessary. For example the words plays, played, playing all come from the base word play. I only wanted the base word. To clean the text, I created a bunch of functions to lowercase the text, expand contractions, remove whitespace and remove stop words. In addition I used two common practices in NLP, stemming and lemmatization to derive the base of each word. Although lemmatization took more time, I found it more effective and chose too keep the lemmatized text reviews over the stemmed ones.\n",
    "\n",
    "__4.3.__ Non-English Text Reviews. Some of the text reviews were not in English. This is an issue because the non-English words would throw the model off. Using lang detect I searched through the data and dropped any rows of text reviews that were not identified as English. Sometimes it was unable to detect the language. For these I used a try and except clause to ignore the reviews that a language could not be identified.\n",
    "\n",
    "To wrap up data wrangling I also did the following to the dataset:\n",
    "+ Took a subset of the entire data collected\n",
    "+ Dropped unnecessary columns: time, beerId, brewerId\n",
    "+ Fixed spelling errors\n",
    "+ Broke down text reviews in preparation for sentiment analysis\n",
    "+ Added a new feature: word_count\n",
    "+ Extracted five beer styles to examine along with the entire dataset: Amber Ale, Belgian Ale, Brown Ale, IPA, Stout\n",
    "+ Started with dataset size of 99,999. After cleaning and removing data that it was reduced to 81048.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 5. Exploratory Data Analysis and Inferential Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5.1. Rating Scales__\n",
    "\n",
    "In total there were 5 rating scales provided from the dataset acquired: Appearance, Aroma, Palate, Taste and Overall. Each one provides its own insight on different qualities of a beer, but for the purpose of this project I focused on the Overall rating by reviewers to get, as the rating is called, \"Overall\" understanding. In addition to examining the entire dataset I selected five beer styles to look at to see if there were any oddities.\n",
    "\n",
    "<img src=\"plots/overall_ratings.png\" alt=\"overall ratings\"/>\n",
    "\n",
    "__Figure 1:__ This shows the distribution of ratings for the entire beer dataset(gold) along with five beer styles: India Pale Ale, Amber Ale, Belgian Ale, Brown Ale and Stout. The white square in the middle of each box plot represents the average amongst all ratings. You can see that for the entire dataset, both the mean and median fall roughly around 3.5. This varies amongst styles of beers but still remains within the 3-4 star range. \n",
    "\n",
    "Since the ratings ranged within the 3-4 star range, I decided to create a binary rating system derived from this plot. Reviews with an overall rating less than 3.5 were considered to be \"low\" and reviews greater than or equal to 3.5 were considered to be \"high\". \n",
    "\n",
    "The most important feature for the model is the text within each review. To further examine this I decided to create a new feature, word count, and examine this further. Does the review length help determine anything within this newly created two-rating system. On initial appearances it appeared that the higher the rating given, the lengthier the review was in terms of words. To further answer this question I performed a two sample t-test for unequal variances. Results showed that the word count of reviews rated higher than 3.5 is significantly higher than reviews rated less than 3.5. \n",
    "\n",
    "<img src=\"plots/high_low_words.png\" alt=\"word_frequency_1\"/>\n",
    "\n",
    "__Figure 2:__ Word counts for reviews greater than 3.5 are higher than word counts for reviews less than 3.5 for the entire dataset and each beer style examined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5.2. Text Reviews__\n",
    "\n",
    "<img src=\"plots/wordfreq.png\" alt=\"word_frequency_1\"/>\n",
    "\n",
    "__Figure 3:__ Top 25 most common words appearing within the dataset.\n",
    "\n",
    "As mentioned earlier, the main features of the model will come from the text reviews. To further examine this I looked at the 25 most common words within the entire corpus. Among this list there are sensory words (aroma, sweet, light, caramel, etc.) that help imagine the beer. There are also words in relation to beer such as \"hop\", \"head\" (in reference to the foam on the top of a beer) and \"pour\". Although text preprocessing was already performed, additional words may need to be removed such as \"beer\" because they do not help in distinguishing reviews from one another. \n",
    "\n",
    "A point of notice is that the word \"not\" appeared quite frequently. When determining sentiment the word \"not\" can change the polarity of the review. For example the difference in the sentences \"This is good\" and \"This is not good\". Below I have  the probability for some words associated with a high and low overall rating. Individual words are in the first set of probabilities. You see words that you would normally see with high and low reviews such as 'excellent' and 'delicious', but you also see noise such as 'xx'. The probabilities of each word are also not that strong.\n",
    "\n",
    "***1 WORD***\n",
    "\n",
    "Good words\t     P(high | word)        \n",
    "+           excellent 0.92\n",
    "+              chimay 0.89\n",
    "+           beautiful 0.88\n",
    "+           delicious 0.87\n",
    "+                rich 0.87\n",
    "+             complex 0.86\n",
    "+                full 0.86\n",
    "+                pine 0.86\n",
    "+          grapefruit 0.86\n",
    "+           wonderful 0.86\n",
    "\n",
    "Bad words\t     P(low | word)\n",
    "+              boring 0.39\n",
    "+           cardboard 0.38\n",
    "+                  xx 0.38\n",
    "+               stale 0.37\n",
    "+              skunky 0.36\n",
    "+               lager 0.36\n",
    "+              watery 0.34\n",
    "+                weak 0.34\n",
    "+                corn 0.30\n",
    "+               bland 0.29\n",
    "\n",
    "In comparison when you look at both one and two word phrases, you begin to get stronger probabilities.\n",
    "\n",
    "***1-2 WORDS***\n",
    "\n",
    "Good words\t     P(high | word)\n",
    "+          great brew 0.99\n",
    "+          delightful 0.99\n",
    "+         chimay beer 0.99\n",
    "+         black thick 0.98\n",
    "+        trappist ale 0.98\n",
    "+         palate full 0.98\n",
    "+            not wait 0.98\n",
    "+        silky smooth 0.98\n",
    "+       complex aroma 0.98\n",
    "+       another solid 0.97\n",
    "\n",
    "Bad words\t     P(low | word)\n",
    "+        light watery 0.08\n",
    "+            horrible 0.08\n",
    "+               nasty 0.07\n",
    "+              skunky 0.07\n",
    "+       grainy flavor 0.06\n",
    "+               awful 0.06\n",
    "+               corny 0.05\n",
    "+          quite thin 0.05\n",
    "+                 meh 0.04\n",
    "+               drain 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 6. Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine learning model aims to take new text reviews and predict either a low or high overall rating. Normally you'd first want to do some text preprocessing to clean the text reviews of nonessential information such as contractions, punctuation and whitespace, but I had already done this when first cleaning the dataset. The following steps were then taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6.1. Vectorizer Selection__\n",
    "Vectorizers encode text data and depending on which method you choose, it is calculated differently. I decided to use two different ones, CountVectorizer and TfidfVectorizer, which are both part of Scikit-learn. \n",
    "1. __CountVectorizer:__ uses a \"bag-of-words\" approach where the text is analyzed by word counts. It counts the occurrence of each token for each review.\n",
    "2. __TfidfVectorizer:__ uses the term frequency(tf) and the inverse document frequency(idf) to create weighted term frequencies. To make more sense of this here is an example. If the word \"IPA\" shows up in all the documents, it wouldn't be that helpful for predictions. This vectorizer will downweight the predictive value of the word \"IPA\" because it does not help us as much. \n",
    "\n",
    "To compare the two vectorizers I applied both vectorizers separately to a simple Naive Bayes classifier, MultinomialNB() including bigrams (ngram_range = (1,2)). I calculated the ROC-AUC score to compare the models. \n",
    "\n",
    "After I changed the minimum document frequency to .0001 instead of 1. I then used GridSearchCV to select the best parameters for the simple Naive Bayes model. This told me that the default parameters for alpha and fit_prior were the best. I then recalculated the ROC-AUC score for both vectorizers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Vectorizer             | ROC-AUC | Best Parameters                       |\n",
    "|------------------------|---------|---------------------------------------|\n",
    "| CountVectorizer        | 0.853   | min_df=1, alpha=1, fit_prior=True     |\n",
    "| TfidfVectorizer        | 0.848   | min_df=1, alpha=1, fit_prior=True     |\n",
    "| CountVec w/ GridSearch | 0.853   | min_df=.0001, alpha=1, fit_prior=True |\n",
    "| TfidfVec w/ GridSearch | 0.857   | min_df=.0001, alpha=1, fit_prior=True |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Table 1:__ Comparison of vectorizers with a Multinomial Naive Bayes Model with and without hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the table above you can see that the TfidfVectorizer worked best with tuned parameters. The following parameters were best and what I used moving forward on other models and classifiers.\n",
    "\n",
    "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
    "       estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
    "       fit_params=None, iid='warn', n_jobs=-1,\n",
    "       param_grid={'fit_prior': (True, False), 'alpha': (0.001, 0.01, 0.1, 1, 5, 10)},\n",
    "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
    "       scoring='roc_auc', verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6.2. Model Comparison__\n",
    "\n",
    "Using the TfidfVectorizer I fit and tuned four classifiers: Logistic Regression, Support Vector Machines, Multinomial Naive Bayes and Random Forest Trees. Each classifier used an ngram_range of (1,2) and a min_df of 0.0001. I used area under the curve(ROC) to compare the models to one another. For Random Forest Trees, and Naive Bayes I used GridSearchCV for cross-validation and hyper parameter tuning. For LogisticRegression I used an algorithm(LogisticRegressionCV) that included cross-validation. Lastly, using SVM, I was able to get the second highest ROC score, but I decided not to work with it further for the time being due to the amount of time it took to run the model. \n",
    "\n",
    "The highest scoring model turned out to be Logistic Regression. The score and parameters of each model are in the table below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Classifier             | ROC-AUC | Best Parameters                       |\n",
    "|------------------------|---------|---------------------------------------|\n",
    "| MultinomialNB          | 0.859   | alpha=1, fit_prior=True               |\n",
    "| LogisticRegressionCV   | 0.873   | Cs=10, max_iter=1000                  |\n",
    "| RandomForestClassifier | 0.851   | max_features=500, min_samples_leaf=5  |\n",
    "| SVC                    | 0.868   | C=1, kernal='linear', degree=3        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Table 2:__ Comparison of four machine learning models fitted with TfidfVectorizer. \n",
    "\n",
    "<img src=\"plots/roc_curve_logreg.png\" alt=\"word_frequency_1\"/>\n",
    "\n",
    "__Figure 4.__ ROC curve for best model, LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6.3. Examine Text, False Positives, False Negatives and Predicted Probabilities__\n",
    "\n",
    "In hopes of further improving the model, I decided to examine the text in more detail. I wanted to see if I could find common characteristics within false positives and false negatives that could be addressed to reduce their occurrences. To do so I examined a couple false positives with the highest predicted probability and a couple false negatives with the lowest predicted probabilities. This way I would be able to examine the extremes of both error groups. Below are two reviews from each. \n",
    "\n",
    "1. False Positives\n",
    "    + 22oz bottle from Bevmo.  Pours a yellowish color with a pure white, fluffy head. Piney citrus aroma. Nice toasty malt flavors behind some citrus hop flavors. Very floral in the finish. Really nice and drinkable, this one has some good character, and hides the alcohol well. A very solid, tasty brew. \n",
    "    + fruity sweet smell with a nice foamy head. good malt with a dry hoppy finish good balance alcohol is well hidden \n",
    "\n",
    "2. False Negatives\n",
    "    + On tap, at brewpub.  Pours a clear pale yellow with a minimal wispy white head, light visible carbonation.  Light but clean pale malt nose, no corn noticeable with minimal to no hops.  The flavors a bit grassy and has a touch of corn, but its still pretty clean.  The mouth feel is light bodied and fizzy wth fizzy spritzy carbonation.  Not bad for what it is. \n",
    "    + 650 mL bottle.  Looks the part, but the aroma is mild, and the flavour is one-dimensional.  Not enough roast and too much soya sauce.  Not bad, really.  Just a little disappointing. \n",
    "\n",
    "For most of each review, the information discussed describes the beer and its qualities, talking about color, taste, appearance, carbonation, etc. However there are portions in each that provide some form of sentiment with words like \"good\" and \"solid\" in the false positives and words like \"disappointing\" and \"not enough\". This may make the reviews appear more negative or positive than they really are.\n",
    "\n",
    "Second, it could be that these reviewers are either very harsh critics or more easy going. Since my binary rating of \"high\" and \"low\" was made at the point 3.5, there may be some who consider an average beer to be 3 stars and other who would put it up at 4. With more data it may be wise to leave the rating system unadjusted. \n",
    "\n",
    "In addition I examined the predicted probabilities of some of these reviews that were predicted incorrectly in the table below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Incorrect Prediction | Index | Actual Rating | Predicted Probability |\n",
    "|----------------------|-------|---------------|-----------------------|\n",
    "| False Positive       | 67135 | low           |0.995780               |\n",
    "| False Positive       | 61414 | low           |0.993783               |\n",
    "| False Positive       | 2222  | low           |0.989848               |\n",
    "| False Negative       | 36260 | high          |0.004538               |\n",
    "| False Negative       | 4757  | high          |0.005078               |\n",
    "| False Negative       | 55204 | high          |0.005502               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Table 3:__ This table shows the false prediction of a review, the index it can be found in the data, the correct rating and the predicted probability using LogisticRegressionCV. \n",
    "\n",
    "The default threshold for the predicted probabilities is 0.5. The model predicts the class that has a probability greater than 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6.4. Adjusting Threshold of Logistic Regression Model for Different Business Models.__\n",
    "\n",
    "Furthermore, another way to examine the performance of the model is to look at the confusion matrix. For the best model, LogisticRegressionCV at a threshold level of 0.5, 9982 reviews were correctly predicted to be 'high' ratings and 6229 were correctly predicted to be 'low' ratings. Likewise, 1762 were incorrectly predicted to be 'high' ratings (false positives) and 2379 were incorrectly predicted to be 'low' ratings (false negatives). \n",
    "\n",
    "However, in real life situations you may require the model to focus on different metrics. You may want the model to be as accurate as it can be or you may determine that focus needs to be put on one category over another. What type of scoring metrics could you use? Well, I used the following metrics.\n",
    "+ Sensitivity (Recall) - The proportion of positives that are correctly predicted \n",
    "+ Precision - The proportion that the prediction will be a positive\n",
    "+ Accuracy - The proportion correct predictions \n",
    "\n",
    "In addition, because the dataset was imbalanced, I looked at the following two metrics to take account for this.\n",
    "+ F1 Score - The harmonic mean between precision and recall.\n",
    "+ Balanced Accuracy - Accuracy determined for imbalanced datasets. \n",
    "\n",
    "These metrics are important for three business cases that could be applied to the beer reviews. They can be further broadened if desired to apply to any business that involves user created reviews.\n",
    "\n",
    "__BUSINESS CASE 1: Make the Model as Accurate as Possible__\n",
    "\n",
    "Let's say after you've created a model you want to predict new user reviews into their binary categories \"low\" or \"high\" overall ratings. To do so you'd want to have a predictor that is accurate as it can get. The metrics I decided to use for this are __accuracy__. Since the dataset is unbalanced, there are more \"high\" overall ratings than \"low\", I also examined __balanced-accuracy__. \n",
    "\n",
    "When taking into account the unbalanced dataset, the best threshold is the default value of 0.5. It is 79% accurate. However, if you want to obtain the most accurate prediction regardless of the imbalance you would want to move the threshold to 0.43. The accuracy improves to 80%, reducing the total number of mispredictions to 4007 from 4147. A key point to make is that although the overall number of mispredictions is reduced, the number of false positives increases and the number of false negatives decreases. The values are described in the table below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Prediction Metric | % Accurate | True Positive | True Negative | False Positive | False Negative |\n",
    "|-------------------|------------|---------------|---------------|----------------|----------------|\n",
    "| Accuracy          | 0.803      | 10537         | 5808          | 2183           | 1824           |\n",
    "| Balanced Accuracy | 0.794      | 9982          | 6229          | 1762           | 2379           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Table 4:__ This provides the confusion matrix and score of the metrics Accuracy and Balanced Accuracy\n",
    "\n",
    "To visually understand this I've also plotted this.\n",
    "\n",
    "<img src=\"plots/accuracy_plot.png\" alt=\"accuracy plot\"/>\n",
    "\n",
    "__Figure 5.__ Accuracy Curve for LogisticRegressionCV with different thresholds\n",
    "\n",
    "__BUSINESS CASE 2: Optimize \"Low\" Overall Ratings__\n",
    "\n",
    "Another possibility could be that a business is looking to see if they may need to expand their customer service team to respond to negative or \"low\" ratings. To do so you'd want to focus and highlight \"low\" reviews. To measure this, I decided to use the F1-Score because it combines the precision and recall meaning the higher the F1-Score the better performance from the model. \n",
    "\n",
    "Since the focus is on the \"low\" ratings or the \"0\" values in the prediction it is important to note that when calculate the f1_score you have to make the parameter pos_label=0. In the opposite, which I will mention momentarily, you may leave it at the default value pos_label=1. \n",
    "\n",
    "After examining various thresholds, the optimal threshold for \"low\" ratings is at 0.54. It outputs an F1-Score of 0.75. \n",
    "\n",
    "In comparison looking at the opposite, the optimal threshold for \"high\" ratings you notice that the change is greater from 0.5 to 0.34. The F1-Score also is higher when optimizing for high ratings at a score of 0.82. This can be seen in the plot below.\n",
    "\n",
    "<img src=\"plots/ratings_plot.png\" alt=\"accuracy plot\"/>\n",
    "\n",
    "__Figure 6.__ Precision, Recall and F1 Curves with Thresholding for \"low\" and \"high\" ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 7. Summary\n",
    "\n",
    "Steps leading to applying various machine learning models involved text preprocessing and the creation of document terms. I had to lowercase all text, remove numbers, punctuation and special characters as well as removing common words. I found that lemmatizing the text was slightly better than stemming. This may change in the future with a bigger dataset because lemmatization did take longer to run. Examining document frequency was better when looking at the weighted proportions(TfidfVectorizer) for words rather than the overall count(CountVectorizer).\n",
    "\n",
    "Out of the four models I chose to use, Logistic Regression appeared to be the most successful and cost efficient to run. Although SVC, may have performed better with hypertuned parameters, the time it took to run the classifier was not worth continuing with it. \n",
    "\n",
    "To further examine the text I looked at false negatives and false positives and their predicted probabilities. In addition I examined different thresholds to optimize correct predictions for both 'low' and 'high' ratings. What I found was that there were quite a lot false negative predictions(2379) and false positive predictions(1762). To continue to explore I decided to examine two different business cases by thresholding.\n",
    "\n",
    "The default threshold was 0.5 meaning any value above that was considered a 'high' rating and any value below was considered 'low'. However, this can be adjusted depending on what you are measuring for. \n",
    "\n",
    "In Business Case 1, I wanted to make the model as accurate as possible. To do so I adjusted the threshold to 0.43 scoring at 80% accuracy. \n",
    "\n",
    "In Business Case 2, I wanted to make the model optimize low ratings. Why? It may be a topic of discussion to increase the customer service staff based on what consumers are saying. Doing so I used the F1-Score as a metric because it combines precision and recall. Again, I adjusted the threshold from 0.5 to 0.54 outputting a F1-Score of 0.75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 8. Recommendations for Future Implementation\n",
    "\n",
    "To conclude my project, I used Logistic Regression to correctly predict as many 'high' and 'low' ratings that I could using cross-validation, thresholding, bi-grams, vectorizers and much more. However, there is still a lot of room for improvement and with the time and resources there are many ways this model can be made to be much better. Here are a few of these topics below.\n",
    "+ Use all data in the dataset. I only used 99,999 reviews throughout the project but the dataset has an additional 1.4 million reviews. Also, use more recent data from any website with beer reviews to match more current events. \n",
    "+ Try and focus on various styles of beers. I examined the IPA beer dataset to see if the overall model would work better on an individual dataset, but it failed to do so. However, there are over a 100 more different styles of beer to be explored and examined. An adjusted model for each style may create an overall better rating predictor.\n",
    "+ There are four other rating scales that can be explored as well. I focused on the \"overall\" rating to get a general understanding of the review,s but there are still \"appearance\", \"aroma\", \"palate\", and \"taste\". In addition I reduced the rating scale from a 5 point scale to a binary scale. With more data it may be a good idea to examine it on the original scale it came on.\n",
    "+ Explore word connections some more\n",
    "    + There are more advanced NLP practices such as Word2Vec that can provide more insight from the text reviews\n",
    "+ I mentioned that I had to stop working with SVC due to time constraints. With more processing power it may be interesting to explore other and more powerful models. It would help work with a bigger dataset as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 9. Sources\n",
    "\n",
    "Lipton, Zachary & Vikram, Sharad & McAuley, Julian. (2015). Capturing Meaning in Product Reviews with Character-Level Generative Text Models.\n",
    "https://www.researchgate.net/publication/283761921_Capturing_Meaning_in_Product_Reviews_with_Character-Level_Generative_Text_Models\n",
    "\n",
    "Springboard Mentors - Benjamin Bell, DJ Sarkar, Kenneth Gil"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
