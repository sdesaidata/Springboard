{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone 1 Final Report: Beer Rating Predictor\n",
    "Written by Soham Desai\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"plots/ipa_wordcloud.png\" alt=\"ipa wordcloud\"/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Word cloud for 'IPA' Beer Style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "1. Introduction\n",
    "2. Client Profile\n",
    "3. Data Information\n",
    "4. Data Wrangling\n",
    "5. Exploratory Data Analysis & Inferential Statistics\n",
    "6. Machine Learning\n",
    "7. Summary\n",
    "8. Recommendations\n",
    "9. Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picture you having a hard day at work where you were stressed over your head. On your way home you decide you deserve a little reward and want to unwind a little by having a beer or two. You stop by at the store or the local pub only to find a selection of beverages you have no idea about. What do you do? You can ask one or two people for a recommendation or better yet you can go search the Internet for reviews and ratings to make a more informed decision.\n",
    "\n",
    "With an increase in craft beers and breweries beer variety has increased dramatically. Due to this over saturation of products it can be very overwhelming to to discover new brands and styles within the beer community. However, the Internet has brought beer connoisseurs from all around together on websites such as BeerAdvocate.com to discuss and review different products. In fact many consumers rely on these reviews to make informed purchases. Not only that, these reviews and ratings, when positive, can help businesses increase revenue by gaining recognition and reputation. It is therefore very valuable for, both breweries and connoisseurs, to have accurate ratings and reviews. I was curious to see is it possible to predict ratings into two categories, high and low, based on the content of the review. Using Natural Language Processing and Sentiment Analysis I used various machine learning models to predict ratings from data I acquired. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 2. Client Profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My main client would be beer brewers at all levels. This information could be valuable for big corporations such as Budweiser all the way down to your local brewery that is about to start up. The machine learning algorithm could be used to sift through their consumer reviews to provide the most accurate and consistent ratings to make these informed decisions on beer to increase sales and customer satisfaction. It can also help shed light at the development phase by showing what styles are popular to see if a new combination could become a big seller.\n",
    "\n",
    "In addition, my algorithm can be expanded upon to be used for any business that has some sort of online database of reviews. With the proper parameters the algorithm can be altered to cater to any sort of business. They would be able to examine the content of the reviews and the derived sentiment to understand user emotions better and improve customer satisfaction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 3. Data Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains over 1.5 million reviews of various beers from two websites: BeerAdvocate.com. This data not only includes user reviews, product category and alcohol by volume(ABV), but sensory aspects as well such as taste, look, smell and overall ratings. For this project I will train and test models to predict beer ratings and beer style based off the user reviews that were left.\n",
    "\n",
    "These reviews were made available by Julian Mcauley, a UCSD Computer Science professor, from a collection period of January 1998 to November 2011. This dataset was accessed with permission. Here are some key specs of the dataset itself.\n",
    "\n",
    "+ Number of users: 33,387\n",
    "+ Number of items: 66,051\n",
    "+ Number of reviews: 1,586,259\n",
    "+ Rating Scales: Appearance, Aroma, Palate, Taste, Overall\n",
    "\n",
    "To tackle the issue of size, I took a subset sample of 99,999 reviews to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 4. Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After acquiring the data and examining it, there were several steps needed to be taken to clean the data before it was ready for analysis and machine learning models. \n",
    "\n",
    "\n",
    "__4.1.__ The different ratings left behind by users were on different scales. Aroma and Taste were on a scale of 10, while Appearance and Palate were on a scale of 5. The overall rating was also on an unknown scale. To remedy this, I scaled down \"Aroma\" and \"Taste\" down to a 5-point scale to match the other. I also combined the four sensory ratings and averaged them to get an \"Overall\" rating.\n",
    "\n",
    "__4.2.__ The text reviews had a bunch of miscellaneous information in them. This included spelling errors, punctuation, stop words, contractions, capitalization and whitespace. Also, having different tenses of a word is not necessary. For example the words plays, played, playing all come from the base word play. I only wanted the base word. To clean the text, I created a bunch of functions to lowercase the text, expand contractions, remove whitespace and remove stop words. In addition I used two common practices in NLP, stemming and lemmatization to derive the base of each word. Although lemmatization took more time, I found it more effective and chose too keep the lemmatized text reviews over the stemmed ones.\n",
    "\n",
    "__4.3.__ Non-English Text Reviews. Some of the text reviews were not in English. This is an issue because the non-English words would throw the model off. Using lang detect I searched through the data and dropped any rows of text reviews that were not identified as English. Sometimes it was unable to detect the language. For these I used a try and except clause to ignore the reviews that a language could not be identified.\n",
    "\n",
    "To wrap up data wrangling I also did the following to the dataset:\n",
    "+ Took a subset of the entire data collected\n",
    "+ Dropped unnecessary columns: time, beerId, brewerId\n",
    "+ Fixed spelling errors\n",
    "+ Broke down text reviews in preparation for sentiment analysis\n",
    "+ Added a new feature: word_count\n",
    "+ Extracted five beer styles to examine along with the entire dataset: Amber Ale, Belgian Ale, Brown Ale, IPA, Stout\n",
    "+ Started with dataset size of 99,999. After cleaning and removing data that it was reduced to 81048.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 5. Exploratory Data Analysis and Inferential Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5.1. Rating Scales__\n",
    "\n",
    "In total there were 5 rating scales provided from the dataset acquired: Appearance, Aroma, Palate, Taste and Overall. Each one provides its own insight on different qualities of a beer, but for the purpose of this project I focused on the Overall rating by reviewers to get, as the rating is called, \"Overall\" understanding. In addition to examining the entire dataset I selected five beer styles to look at to see if there were any oddities.\n",
    "\n",
    "<img src=\"plots/overall_ratings.png\" alt=\"overall ratings\"/>\n",
    "\n",
    "__Figure 1:__ This shows the distribution of ratings for the entire beer dataset(gold) along with five beer styles: India Pale Ale, Amber Ale, Belgian Ale, Brown Ale and Stout. The white square in the middle of each box plot represents the average amongst all ratings. You can see that for the entire dataset, both the mean and median fall roughly around 3.5. This varies amongst styles of beers but still remains within the 3-4 star range. \n",
    "\n",
    "Since the ratings ranged within the 3-4 star range, I decided to create a binary rating system derived from this plot. Reviews with an overall rating less than 3.5 were considered to be \"low\" and reviews greater than or equal to 3.5 were considered to be \"high\". \n",
    "\n",
    "The most important feature for the model is the text within each review. To further examine this I decided to create a new feature, word count, and examine this further. Does the review length help determine anything within this newly created two-rating system. On initial appearances it appeared that the higher the rating given, the lengthier the review was in terms of words. To further answer this question I performed a two sample t-test for unequal variances. Results showed that the word count of reviews rated higher than 3.5 is significantly higher than reviews rated less than 3.5. \n",
    "\n",
    "<img src=\"plots/high_low_words.png\" alt=\"word_frequency_1\"/>\n",
    "\n",
    "__Figure 2:__ Word counts for reviews greater than 3.5 are higher than word counts for reviews less than 3.5 for the entire dataset and each beer style examined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5.2. Text Reviews__\n",
    "\n",
    "<img src=\"plots/wordfreq.png\" alt=\"word_frequency_1\"/>\n",
    "\n",
    "__Figure 3:__ Top 25 most common words appearing within the dataset.\n",
    "\n",
    "As mentioned earlier, the main features of the model will come from the text reviews. To further examine this I looked at the 25 most common words within the entire corpus. Among this list there are sensory words (aroma, sweet, light, caramel, etc.) that help imagine the beer. There are also words in relation to beer such as \"hop\", \"head\" (in reference to the foam on the top of a beer) and \"pour\". Although text preprocessing was already performed, additional words may need to be removed such as \"beer\" because they do not help in distinguishing reviews from one another. \n",
    "\n",
    "A point of notice is that the word \"not\" appeared quite frequently. When determining sentiment the word \"not\" can change the polarity of the review. For example the difference in the sentences \"This is good\" and \"This is not good\". Below I have  the probability for some words associated with a high and low overall rating. Individual words are in the first set of probabilities. You see words that you would normally see with high and low reviews such as 'excellent' and 'delicious', but you also see noise such as 'xx'. The probabilities of each word are also not that strong.\n",
    "\n",
    "***1 WORD***\n",
    "\n",
    "Good words\t     P(high | word)        \n",
    "+           excellent 0.92\n",
    "+              chimay 0.89\n",
    "+           beautiful 0.88\n",
    "+           delicious 0.87\n",
    "+                rich 0.87\n",
    "+             complex 0.86\n",
    "+                full 0.86\n",
    "+                pine 0.86\n",
    "+          grapefruit 0.86\n",
    "+           wonderful 0.86\n",
    "\n",
    "Bad words\t     P(low | word)\n",
    "+              boring 0.39\n",
    "+           cardboard 0.38\n",
    "+                  xx 0.38\n",
    "+               stale 0.37\n",
    "+              skunky 0.36\n",
    "+               lager 0.36\n",
    "+              watery 0.34\n",
    "+                weak 0.34\n",
    "+                corn 0.30\n",
    "+               bland 0.29\n",
    "\n",
    "In comparison when you look at both one and two word phrases, you begin to get stronger probabilities.\n",
    "\n",
    "***2 WORDS***\n",
    "\n",
    "Good words\t     P(high | word)\n",
    "+          great brew 0.99\n",
    "+          delightful 0.99\n",
    "+         chimay beer 0.99\n",
    "+         black thick 0.98\n",
    "+        trappist ale 0.98\n",
    "+         palate full 0.98\n",
    "+            not wait 0.98\n",
    "+        silky smooth 0.98\n",
    "+       complex aroma 0.98\n",
    "+       another solid 0.97\n",
    "\n",
    "Bad words\t     P(low | word)\n",
    "+        light watery 0.08\n",
    "+            horrible 0.08\n",
    "+               nasty 0.07\n",
    "+              skunky 0.07\n",
    "+       grainy flavor 0.06\n",
    "+               awful 0.06\n",
    "+               corny 0.05\n",
    "+          quite thin 0.05\n",
    "+                 meh 0.04\n",
    "+               drain 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 6. Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine learning model aims to take new text reviews and predict either a low or high overall rating. Normally you'd first want to do some text preprocessing to clean the text reviews of nonessential information such as contractions, punctuation and whitespace, but I had already done this when first cleaning the dataset. The following steps were then taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6.1. Vectorizer Selection__\n",
    "Vectorizers encode text data and depending on which method you choose, it is calculated differently. I decided to use two different ones, CountVectorizer and TfidfVectorizer, which are both part of Scikit-learn. \n",
    "1. __CountVectorizer:__ uses a \"bag-of-words\" approach where the text is analyzed by word counts. It counts the occurrence of each token for each review.\n",
    "2. __TfidfVectorizer:__ uses the term frequency(tf) and the inverse document frequency(idf) to create weighted term frequencies. To make more sense of this here is an example. If the word \"IPA\" shows up in all the documents, it wouldn't be that helpful for predictions. This vectorizer will downweight the predictive value of the word \"IPA\" because it does not help us as much. \n",
    "\n",
    "To compare the two vectorizers I applied both vectorizers separately to a simple Naive Bayes classifier, MultinomialNB() including bigrams (ngram_range = (1,2)). I calculated the ROC-AUC score to compare the models. \n",
    "\n",
    "After I changed the minimum document frequency to .0001 instead of 1. I then used GridSearchCV to select the best parameters for the simple Naive Bayes model. This told me that the default parameters for alpha and fit_prior were the best. I then recalculated the ROC-AUC score for both vectorizers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Vectorizer             | ROC-AUC | Best Parameters                       |\n",
    "|------------------------|---------|---------------------------------------|\n",
    "| CountVectorizer        | 0.853   | min_df=1, alpha=1, fit_prior=True     |\n",
    "| TfidfVectorizer        | 0.848   | min_df=1, alpha=1, fit_prior=True     |\n",
    "| CountVec w/ GridSearch | 0.853   | min_df=.0001, alpha=1, fit_prior=True |\n",
    "| TfidfVec w/ GridSearch | 0.857   | min_df=.0001, alpha=1, fit_prior=True |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Table 1:__ Comparison of vectorizers with a Multinomial Naive Bayes Model with and without hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the table above you can see that the TfidfVectorizer worked best with tuned parameters. The following parameters were best and what I used moving forward on other models and classifiers.\n",
    "\n",
    "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
    "       estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
    "       fit_params=None, iid='warn', n_jobs=-1,\n",
    "       param_grid={'fit_prior': (True, False), 'alpha': (0.001, 0.01, 0.1, 1, 5, 10)},\n",
    "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
    "       scoring='roc_auc', verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6.2. Model Comparison__\n",
    "\n",
    "Using the TfidfVectorizer I fit and tuned four classifiers: Logistic Regression, Support Vector Machines, Multinomial Naive Bayes and Random Forest Trees. Each classifier used an ngram_range of (1,2) and a min_df of 0.0001. I used area under the curve(ROC) to compare the models to one another. For Random Forest Trees, and Naive Bayes I used GridSearchCV for cross-validation and hyper parameter tuning. For LogisticRegression I used an algorithm(LogisticRegressionCV) that included cross-validation. Lastly, using SVM, I was able to get the second highest ROC score, but I decided not to work with it further for the time being due to the amount of time it took to run the model. \n",
    "\n",
    "The highest scoring model turned out to be Logistic Regression. The score and parameters of each model are in the table below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Classifier             | ROC-AUC | Best Parameters                       |\n",
    "|------------------------|---------|---------------------------------------|\n",
    "| MultinomialNB          | 0.859   | alpha=1, fit_prior=True               |\n",
    "| LogisticRegressionCV   | 0.873   | Cs=10, max_iter=1000                  |\n",
    "| RandomForestClassifier | 0.851   | max_features=500, min_samples_leaf=5  |\n",
    "| SVC                    | 0.868   | C=1, kernal='linear', degree=3        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Table 2:__ Comparison of four machine learning models fitted with TfidfVectorizer. \n",
    "\n",
    "<img src=\"plots/roc_curve_logreg.png\" alt=\"word_frequency_1\"/>\n",
    "\n",
    "__Figure 4.__ ROC curve for best model, LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6.3. Examine Text, False Positives, False Negatives and Predicted Probabilities__\n",
    "\n",
    "In hopes of further improving the model, I decided to examine the text in more detail. I wanted to see if I could find common characteristics within false positives and false negatives that could be addressed to reduce their occurrences. Here are two examples of false positives and false negatives\n",
    "\n",
    "1. False Positives\n",
    "    + Dark brown in color with a beige head. Aroma is of sweet malt, brown sugar, and chocolate. Taste is of chocolate and toffee. \n",
    "    + 22 oz bottle.  Pours a light golden amber with a light off tan wispy head. Heavy hop nose with a slightly sweet smell. Medium body. Good hop taste not overpowering, a good balance with the malt flavor. \n",
    "\n",
    "2. False Negatives\n",
    "    + 12 oz. Bottle. Pours a clear straw yellow with a quicky dissipating white head and soapy lacing. Nice malt and hop aroma in the nose. Some diacetyl and sulfur noted. The taste has smooth malt up front with a nice hop profile in the finish. Well made beer. \n",
    "    + Draft @ Beveridge Place Pub, Seattle, WA. Pours a dark brown black color with a small off-white head. Has a roasted malty chocolate aroma. Roasted malty slightly burning hot tabsco and chocolate flavor. Has a long roasted malty burning tabasco finish. \n",
    "\n",
    "Looking at these set of reviews you may notice that they are fairly neutral in sentiment. There are some positives and some negatives within each review, but the majority of each review describes characteristics of the beer. Since I created a binary rating of low and high, these may be at closer to the value I created a break at of 3.5.\n",
    "\n",
    "In addition I examined the predicted probabilities of some of these reviews that were predicted incorrectly in the table below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Incorrect Prediction | Index | Actual Rating | Predicted Probability [low, high] |\n",
    "|----------------------|-------|---------------|-----------------------------------|\n",
    "| False Positive       | 74219 | low           |[0.10, 0.90]                       |\n",
    "| False Positive       | 60307 | low           |[0.44, 0.56]                       |\n",
    "| False Positive       | 56063 | low           |[0.27, 0.73]                       |\n",
    "| False Negative       | 66278 | high          |[0.67, 0.33]                       |\n",
    "| False Negative       | 39671 | high          |[0.89, 0.11]                       |\n",
    "| False Negative       | 77084 | high          |[0.54, 0.46]                       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Table 3:__ This table shows the false prediction of a review, the index it can be found in the data, the correct rating and the predicted probability using LogisticRegressionCV. \n",
    "\n",
    "The default threshold for the predicted probabilities is 0.5. The model predicts the class that has a probability greater than 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6.4. Adjusting Threshold of Logistic Regression Model__\n",
    "\n",
    "Furthermore, another way to examine the performance of the model is to look at the confusion matrix. For the best model, LogisticRegressionCV, 9982 reviews were correctly predicted to be 'high' ratings and 6229 were correctly predicted to be 'low' ratings. Likewise, 1762 were incorrectly predicted to be 'high' ratings (false positives) and 2379 were incorrectly predicted to be 'low' ratings (false negatives). How can you use these values to create scoring metrics? Well, I used the following metrics.\n",
    "+ Sensitivity (Recall) - The proportion of positives that are correctly predicted \n",
    "+ Specificity - The proportion of negatives that are correctly predicted\n",
    "+ Precision - The proportion that the prediction will be a positive\n",
    "\n",
    "Useful information can be obtained from both negative and positive reviews so ideally I'd want to adjust my model to maximize sensitivity and specificity. However, doing so requires a trade off between precision and recall. Seen in the figure below, for logistic regression the precision and recall is inversely correlated. Since the number of high reviews are slighter larger than the number of low reviews I will put a slight emphasis on precision to determine a threshold that will minimize total misclassification of reviews. \n",
    "\n",
    "<img src=\"plots/precision_recall_curve.png\" alt=\"precision recall curve\"/>\n",
    "\n",
    "__Figure 5.__ Precision Recall Curve for LogisticRegressionCV \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the best threshold that would minimize the number of false positives and false negatives I examined various numbers. First I examined the default threshold of 0.50 as a baseline. There is nothing new in this plot that hasn't been discussed before. The total number of incorrect predictions is 4141.\n",
    "\n",
    "<img src=\"plots/Threshold50.png\" alt=\"precision recall curve Threshold .50\"/>\n",
    "\n",
    "__Figure 6.__ Confusion Matrix and Precision Recall Curve at the threshold level of 0.5 for the Logistic Regression Model\n",
    "\n",
    "After I decided to look at number higher so I moved the threshold level to 0.66. This increased the overall number of incorrect predictions from 4141 to 5031 so this isn't what I was looking for. However it is a point to be made that increasing the threshold decreased the number of false positives. If were to come back to focus only on 'high' ratings, I would look into increasing the threshold.\n",
    "\n",
    "<img src=\"plots/Threshold66.png\" alt=\"precision recall curve Threshold .66\"/>\n",
    "\n",
    "__Figure 7.__ Confusion Matrix and Precision Recall Curve at the threshold level of 0.66 for the Logistic Regression Model\n",
    "\n",
    "When lowering the threshold to 0.33, it had inverse effects. Doing so decreased the number of false negative predictions. It also was slightly better than the default threshold of 0.5 decreasing the total number of incorrect predictions from 4141 to 4120. For further analysis, I experimented with other values for the threshold. \n",
    "\n",
    "<img src=\"plots/Threshold33.png\" alt=\"precision recall curve Threshold .33\"/>\n",
    "\n",
    "__Figure 8.__ Confusion Matrix and Precision Recall Curve at the threshold level of 0.33 for the Logistic Regression Model\n",
    "\n",
    "After experimenting with different values for the threshold, I found that the value of 0.45 had the least overall number of incorrect predictions. The total number of incorrect predictions was 4014 with 1961 false negatives and 2053 false positives. \n",
    "\n",
    "<img src=\"plots/Threshold45.png\" alt=\"precision recall curve Threshold .45\"/>\n",
    "\n",
    "__Figure 9.__ Confusion Matrix and Precision Recall Curve at the threshold level of 0.45 for the Logistic Regression Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 7. Summary\n",
    "\n",
    "Steps leading to applying various machine learning models involved text preprocessing and the creation of document terms. I had to lowercase all text, remove numbers, punctuation and special characters as well as removing common words. I found that lemmatizing the text was slightly better than stemming. This may change in the future with a bigger dataset because lemmatization did take longer to run. Examining document frequency was better when looking at the weighted proportions(TfidfVectorizer) for words rather than the overall count(CountVectorizer).\n",
    "\n",
    "Out of the four models I chose to use, Logistic Regression appeared to be the most successful and cost efficient to run. Although SVC, may have performed better with hypertuned parameters, the time it took to run the classifier was not worth continuing with it. \n",
    "\n",
    "To further examine the text I looked at false negatives and false positives and their predicted probabilities. In addition I examined different thresholds to optimize correct predictions for both 'low' and 'high' ratings. What I found was that there were quite a lot false negative predictions(2379) and false positive predictions(1762). To further explore this I decided to examine different thresholds.\n",
    "\n",
    "The default threshold was 0.5 meaning any value above that was considered a 'high' rating and any value below was considered 'low'. Since I wanted to focus on reducing the total number of incorrect predictions I came to the value of 0.45 for the threshold. However, from future exploration if I wanted to focus on either group, false negatives or false positives, I would focus on decreasing and increasing the threshold respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 8. Recommendations for Future Implementation\n",
    "\n",
    "To conclude my project, I used Logistic Regression to correctly predict as many 'high' and 'low' ratings that I could using cross-validation, thresholding, bi-grams, vectorizers and much more. However, there is still a lot of room for improvement and with the time and resources there are many ways this model can be made to be much better. Here are a few of these topics below.\n",
    "+ Use all data in the dataset. I only used 99,999 reviews throughout the project but the dataset has an additional 1.4 million reviews. Also, use more recent data from any website with beer reviews to match more current events. \n",
    "+ Try and focus on various styles of beers. I examined the IPA beer dataset to see if the overall model would work better on an individual dataset, but it failed to do so. However, there are over a 100 more different styles of beer to be explored and examined. An adjusted model for each style may create an overall better rating predictor.\n",
    "+ There are four other rating scales that can be explored as well. I focused on the \"overall\" rating to get a general understanding of the review,s but there are still \"appearance\", \"aroma\", \"palate\", and \"taste\". In addition I reduced the rating scale from a 5 point scale to a binary scale. With more data it may be a good idea to examine it on the original scale it came on.\n",
    "+ Using Word2Vec explore the words some more\n",
    "+ I mentioned that I had to stop working with SVC due to time constraints. With more processing power it may be interesting to explore other and more powerful models. It would help work with a bigger dataset as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 9. Sources\n",
    "\n",
    "Lipton, Zachary & Vikram, Sharad & McAuley, Julian. (2015). Capturing Meaning in Product Reviews with Character-Level Generative Text Models.\n",
    "https://www.researchgate.net/publication/283761921_Capturing_Meaning_in_Product_Reviews_with_Character-Level_Generative_Text_Models\n",
    "\n",
    "Springboard Mentors - Benjamin Bell, DJ Sarkar, Kenneth Gil"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
